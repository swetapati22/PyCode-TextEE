# ğŸ§  Annotation Guidelines xxx PyCode-TextEE 
> Efficient and extensible Event Extraction with prompt optimization â€” built on top of [TextEE](https://github.com/ej0cl6/TextEE)
> This repository includes (1) PyCode-textEE .. code format... built on top of [TextEE](https://github.com/ej0cl6/TextEE) and (2) source code to reproduce [our work on annotation guideline-improved XXX](URL).
>
> If you find out work helpful, please cite:
> bib


---

<div align="center">

[![ğŸ› ï¸ Updates](https://img.shields.io/badge/ğŸ› ï¸%20Updates-Click%20Here-informational?style=flat-square)](#updates) â€¢
[![ğŸ“‚ Datasets](https://img.shields.io/badge/ğŸ“‚%20Datasets-15%20Supported-brightgreen?style=flat-square)](#datasets) â€¢
[![âš™ï¸ Environment](https://img.shields.io/badge/âš™ï¸%20Environment-Setup-orange?style=flat-square)](#environment) â€¢
[![ğŸš€ Running](https://img.shields.io/badge/ğŸš€%20Running-Instructions-blueviolet?style=flat-square)](#running) â€¢
[![ğŸŒ Website](https://img.shields.io/badge/ğŸŒ%20Website-Demo%20Soon-lightgrey?style=flat-square)](#website) â€¢
[![ğŸ“„ Paper](https://img.shields.io/badge/ğŸ“„%20arXiv-2502.16377-b31b1b?style=flat-square)](https://arxiv.org/abs/2502.16377)


</div>

---

**Authors**:  
Saurabh Srivastava, Sweta Pati, Ziyu Yao

---

## ğŸ§© Introduction

**PyCode-TextEE** extends [TextEE](https://github.com/ej0cl6/TextEE), bringing **event extraction into the era of prompt-based large language models**.

While **TextEE** standardizes 10+ event extraction datasets into a unified JSON formatâ€”making them reproducible and comparableâ€”**PyCode-TextEE** takes the next leap:

> âœ¨ We transform TextEE-formatted data into **code-style prompts**â€”a format that is both readable and executable by LLMs and ideal for structured evaluation.

TODO: add some background about annotation guidelines.
add a screenshot showing 1) python code 2) what's annotation guideline
point people to paper again


---

### ğŸš€ Whatâ€™s New in PyCode-TextEE?

- **CodePrompt Format Conversion**  
  We convert event structures (event triggers, argumentsâ€”if available) into Python-like prompts (e.g., `Attack(mention="...", attacker=[...], target=[...])`) to help LLMs handle structured outputs.

- **annotation guidelines**
...
  
- **Plug-and-Play with TextEE**  
  Directly load standardized datasets from TextEE and transform them with one command into training-ready CodePrompts.

- **Evaluation Toolkit for Prompted LLMs**  
  We provide exact-match evaluation utilities that compute **precision, recall, and F1 scores** over structured LLM outputs.

- **Code to produce XXXX**  
  Includes all data transformations and training scripts used in a paper currently under submission to a major NLP conference. Code for that will live in `LLaMAEvents/`.

<a id="updates"></a>
## ğŸ› ï¸ Updates

- **April 23, 2025** â€” We release **PyCode-TextEE**, a modular framework for converting standardized event extraction datasets (via TextEE) into code-style prompts, along with exact-match evaluation scripts.  
  Feel free to reach out if youâ€™d like to contribute your **models**, **datasets**, or ideas!

<a id="datasets"></a>
## ğŸ“‚ Supported Datasets

We support **15 datasets** for Event Detection (ED), Event Argument Extraction (EAE), and End-to-End (E2E) Event Extraction.  
All are converted into **code-style prompts** and support evaluation using our exact-match metric suite.  
The table below also shows whether annotation **guidelines** are included for each dataset.

<div align="center">
<table>
<thead>
<tr>
  <th><strong>Dataset</strong></th>
  <th><strong>Task(s)</strong></th>
  <th><strong>Paper Title</strong></th>
  <th><strong>Source</strong></th>
  <th><strong>Guidelines</strong></th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>ACE05</code></td>
  <td>ED, EAE, E2E</td>
  <td>The Automatic Content Extraction (ACE) Program</td>
  <td><a href="https://www.ldc.upenn.edu/">LDC</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>ERE</code></td>
  <td>ED, EAE, E2E</td>
  <td>From Light to Rich ERE</td>
  <td><a href="https://www.ldc.upenn.edu/">LDC</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>MLEE</code></td>
  <td>ED, EAE, E2E</td>
  <td>Biological Event Extraction</td>
  <td><a href="https://academic.oup.com/bioinformatics/article/28/18/i575/245077">Bioinformatics</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>Genia2011</code></td>
  <td>ED, EAE, E2E</td>
  <td>Genia Event Task (2011)</td>
  <td><a href="https://www.aclweb.org/anthology/W11-1801/">BioNLP 2011</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>Genia2013</code></td>
  <td>ED, EAE, E2E</td>
  <td>Genia Event Task (2013)</td>
  <td><a href="https://www.aclweb.org/anthology/W13-5201/">BioNLP 2013</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>M2E2</code></td>
  <td>ED, EAE, E2E</td>
  <td>Cross-media Structured Common Space</td>
  <td><a href="https://aclanthology.org/2020.acl-main.188/">ACL 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>CASIE</code></td>
  <td>ED, EAE, E2E</td>
  <td>CASIE: Cybersecurity Event Extraction</td>
  <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6155">AAAI 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>PHEE</code></td>
  <td>ED, EAE, E2E</td>
  <td>Pharmacovigilance Event Extraction</td>
  <td><a href="https://aclanthology.org/2022.emnlp-main.343/">EMNLP 2022</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>MEE</code></td>
  <td>ED</td>
  <td>Multilingual Event Extraction</td>
  <td><a href="https://aclanthology.org/2022.emnlp-main.428/">EMNLP 2022</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>FewEvent</code></td>
  <td>ED</td>
  <td>Few-Shot Event Detection</td>
  <td><a href="https://dl.acm.org/doi/10.1145/3336191.3371962">WSDM 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>MAVEN</code></td>
  <td>ED</td>
  <td>Massive General-Domain ED</td>
  <td><a href="https://aclanthology.org/2020.emnlp-main.154/">EMNLP 2020</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>SPPED</code></td>
  <td>ED</td>
  <td>ED from Social Media for Epidemic Prediction</td>
  <td><a href="https://aclanthology.org/2024.naacl-main.172/">NAACL 2024</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>MUC-4</code></td>
  <td>EAE</td>
  <td>Fourth Message Understanding Conference</td>
  <td><a href="https://www-nlpir.nist.gov/related_projects/muc/">MUC 1992</a></td>
  <td>âšªï¸</td>
</tr>
<tr>
  <td><code>RAMS</code></td>
  <td>EAE</td>
  <td>Multi-Sentence Argument Linking</td>
  <td><a href="https://aclanthology.org/2020.acl-main.743/">ACL 2020</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>WikiEvents</code></td>
  <td>EAE</td>
  <td>Conditional Generation for Doc-level EAE</td>
  <td><a href="https://aclanthology.org/2021.naacl-main.417/">NAACL 2021</a></td>
  <td>ğŸ”˜</td>
</tr>
<tr>
  <td><code>GENEVA</code></td>
  <td>EAE</td>
  <td>Benchmarking Generalizability for EAE</td>
  <td><a href="https://aclanthology.org/2023.acl-long.794/">ACL 2023</a></td>
  <td>ğŸ”˜</td>
</tr>
</tbody>
</table>


</div>

<a id="environment"></a>
## âš™ï¸ Environment

Although there is no need of any additional package to run PyCode-TextEE, we recommend using **Python 3.9+** with a clean virtual environment (e.g., via `venv` or `conda`).

### ğŸ”¹ Install Dependencies
```bash
# Clone the repo
git clone https://github.com/yourname/PyCode-TextEE.git
cd PyCode-TextEE

# Create a virtual environment (optional)
python3 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate

# Install requirements (optional)
pip install -r requirements.txt
```

### ğŸ”¹ Core Dependencies
> These are the optional dependencies to run the code.
- `datasets`  
- `openai`  
- `wandb` (optional for experiment tracking)  

### âš ï¸ Note
Some datasets (e.g., ACE, ERE) require **LDC license** to access raw files.  We provide code for preprocessing them, but not the data itself.

<a id="running"></a>
## ğŸš€ Running the Code

Below is a step-by-step guide to run PyCode-TextEE.  
Our pipeline is divided into two main stages:

---

### Step 0


### ğŸŒ± Step 1 â€” [Optional] Generate Code Schema

If you're working with custom datasets (or want to regenerate schemas for the 15 supported ones), you'll first convert them into **TextEE format** and generate the corresponding **Python-style event definitions**.

ğŸ“ Directory structure:
```
PyCode-TextEE/
â”œâ”€â”€ code_schema_generation/
â”‚   â”œâ”€â”€ generate_schema.py
â”‚   â”œâ”€â”€ init_prompts/            # Contains per-dataset class schemas (*.txt)
â”‚   â”œâ”€â”€ python_event_defs/       # Python classes for eval (dataset-wise + all_ee_definitions.py)
â”‚   â”œâ”€â”€ mapper.json              # Maps cleaned names â†’ class names
â”‚   â””â”€â”€ schema.json              # All cleaned event/arg schemas
```

ğŸ‘¾ Example output schema (for ACE05 `Attack` event):
```python
@dataclass
class Attack(ConflictEvent):
    mention: str
    target: List
    victim: List
    attacker: List
    instrument: List
    place: List
    agent: List
```

ğŸ›  To generate schema:
```bash
cd code_schema_generation
python generate_schema.py --dataset_folder <your_dataset_dir>
```

ğŸ“‚ Expected dataset layout:
```
<your_dataset_dir>/
â”œâ”€â”€ ace05-en/
â”‚   â”œâ”€â”€ split1/
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”œâ”€â”€ dev.json
â”‚   â”‚   â””â”€â”€ test.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ casie/
â””â”€â”€ ...
```

**Note**: Weâ€™ve already generated schema for all 15 supported datasets. This step is only required for new datasets.

---

### ğŸ”¹ Step 2: Convert to Code Prompts
We first need to make sure that python event definitions are in current environment to verify code prompts.
```bash
cd python_event_defs # this directory is already included in the code or can be generated using Step 1. You can find it in "PyCode-TextEE/code_schema_generation/python_event_defs"
export PYCODE_HOME=$(pwd)
export PYTHONPATH=$PYCODE_HOME:$PYCODE_HOME:$PYTHONPATH
cd ../../ # redirect the terminal to PyCode home directory
```
Run the following:

TODO: placeholders for dataset name and guideline-file
TODO: clarify that --annotate_schema is optional

```bash
cd code_prompts
python prepare_dataset.py \
    --input_dir ../../TextEE/processed_data \
    --dataset_name ace05-en \
    --annotate_schema True \
    --guideline_file ace05-en \
    --add_negative_samples True \
    --output_dir ./processed_code_prompts/
```

---

### âš™ï¸ Argument Descriptions

| Argument               | Description |
|------------------------|-------------|
| `--input_dir`          | Path to TextEE-formatted JSONs (default: `../../TextEE/processed_data`) |
| `--dataset_name`       | Name of the dataset to process (e.g., `ace05-en`) |
| `--annotate_schema`    | Add class docstrings and inline comments using guidelines (default: `False`) |
| `--guideline_file`     | Guideline JSON file for schema annotation (required if `annotate_schema=True`) |
| `--add_negative_samples` | Add negative examples to training set (default: `False`) |
| `--output_dir`         | Where to save the converted code prompts (default: `./processed_code_prompts/`) |

---

### ğŸ§¬ Annotated Prompt Example (with Guidelines)

When `--annotate_schema=True`, we generate prompts like:

```python
@dataclass
class Event(ParentEvent):
    """the event definition"""
    mention: str  # Event trigger definition
    arg_1: List   # Definition of argument 1
    arg_2: List   # Definition of argument 2
```

This format supports **LLM-compatible** structure learning and improves interpretability.

---

### ğŸ“˜ Guideline File Format

Your `guideline_file` should look like:

```json
{
  "EventName1": {
    "description": [
      "One possible definition.",
      "Another variation of the same."
    ],
    "attributes": {
      "mention": "Trigger span of the event.",
      "arg_1": ["One definition for arg_1", "another definition for arg_1"]
    }
  }
}
```

This enables *randomized sampling* during conversion to avoid overfitting to one phrasingâ€”an approach highlighted in our paper.

---

### ğŸ’¡ Tip
â“µ Skip `--guideline_file` and `--annotate_schema` if you're only interested in raw code prompts.  
â“¶ Use `--add_negative_samples` if you want to add negative sample per instance similar to [DEGREE](https://github.com/PlusLabNLP/DEGREE).


## ğŸ§ª Evaluation

Once you've trained your model to generate Python-style event prompts, you can use our evaluation suite in `code_evaluation/` to compute standard **precision, recall, and F1 scores** via exact-match comparison of predicted and gold structured outputs.

### ğŸ“ Directory Overview
```
code_evaluation/
â”œâ”€â”€ all_ee_definitions.py     # Event classes copied from schema generation (Step 1)
â”œâ”€â”€ event_scorer.py           # ğŸ”¥ Main evaluation logic
â”œâ”€â”€ utils_typing.py           # (Attribution to GoLLIE â€” type helper module)
```

---

### ğŸ“Š `event_scorer.py`: Evaluation in a Nutshell

The core script compares model-generated code prompts with gold ones using Python object introspection.

#### âœ… Key Features:
- Extracts arguments from predicted and gold event objects
- Computes **micro/macro F1** across all examples
- Identifies:
  - **Trigger-level mismatches**
  - **Argument-level hallucinations**
- Logs detailed stats (TP / FP / FN per role)

#### ğŸ¯ Core Functions:
- `compute_f1(...)`: calculates precision, recall, and F1 from match counts
- `extract_objects(...)`: extracts fields except for `mention` to compare arguments
- `micro_ed_scores`: calculate micro f1 score on Event Detection task
- `micro_eae_scores`: calculate micro f1 score on Event Argument Extraction task
- `micro_e2e_scores`: calculate micro f1 score on End-to-End Event Extraction task
- `log_hallucinations_and_mismatches(...)`: logs mismatches like hallucinated roles

---

### ğŸ§ª Run the Demo Evaluation

We provide a ready-to-run example in:

```
demo/e2e_demo.json
```

This file contains three illustrative cases:
 \- âœ… One fully correct prediction - ğŸŸ¡ One partially correct - âŒ One incorrect

To run the evaluation:

```bash
cd code_evaluation
python event_scorer.py --input_file ./../demo/e2e_demo.json
```

