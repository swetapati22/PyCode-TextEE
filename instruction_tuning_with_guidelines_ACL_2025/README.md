# Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines

**Authors**: [Saurabh Srivastava](mailto:ssrivas6@gmu.edu), [Sweta Pati](mailto:spati@gmu.edu), [Ziyu Yao](mailto:ziyuyao@gmu.edu)  
**Institution**: George Mason University

---

## Introduction
ğŸ“„ Our paper Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines provides a framework for exploring how annotation guidelines can improve instruction-tuned large language models (LLMs) for event extraction (EE).

- Integrates annotation guidelines (both human and machine-generated) into instruction-tuning.
- Supports **datasets** preprocessed in TextEE's natural language format.
- Provides standardized scripts to convert natural language examples into structured Python code prompts.
- Formats both the **input prompt** and the **modelâ€™s expected output** as structured Python code.
- Enables automatic guideline generation via GPT-based prompt templates.
- Improves performance in low-resource and cross-schema generalization settings.

For more details, please refer to our paper:  
[Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines](https://arxiv.org/abs/2502.16377).

---

## ğŸ—‚ï¸ Repository Structure

```
.
â”œâ”€â”€ data/               # ACE05 and RichERE datasets (processed format)
â”œâ”€â”€ scripts/            # Preprocessing, training, evaluation, and generation
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ generate_guidelines/
â”œâ”€â”€ models/             # LLaMA checkpoints and config files
â”œâ”€â”€ prompts/            # Prompt templates for generating guidelines
â”œâ”€â”€ results/            # Logs, figures, evaluation outputs
â”œâ”€â”€ notebooks/          # Optional: Exploratory notebooks
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ“Š Datasets

We use two widely-used event extraction datasets:

| Dataset   | Event Types | Role Types | Instances (train/dev/test) |
|-----------|-------------|-------------|-----------------------------|
| ACE05     | 33          | 22          | 16,531 / 1,870 / 2,519      |
| RichERE   | 38          | 35          | 9,105 / 973 / 1,163         |

We follow the [TextEE](https://arxiv.org/abs/2311.09562) preprocessing pipeline and provide scripts to convert these preprocessed datasets from TextEEâ€™s natural language format into structured Python code prompts.

To convert your data:
1. Place the preprocessed TextEE-style datasets under the `data/` directory.
2. Follow the detailed steps in [`data/README.md`](data/README.md).

âš ï¸ Note: Access to ACE05 and RichERE requires a license from the [LDC](https://www.ldc.upenn.edu/). We do not redistribute these datasets.

- ACE05 ([Link to ACE05 Dataset](https://catalog.ldc.upenn.edu/LDC2006T06))
- RichERE ([Link to RichERE Dataset](https://catalog.ldc.upenn.edu/LDC2023T04))

---

### âš¡ Guideline Generation

To generate machine-generated annotation guidelines:

```bash
python scripts/guidelines/generate_guidelines.py --dataset ACE05
```

We support multiple prompt variants: `Guideline-P`, `Guideline-PN`, `Guideline-PS`, and their integrated forms. These guidelines are generated using GPT-4 based on positive and negative event examples.

For guideline generation, see [`scripts/guidelines/README.md`](scripts/guidelines/README.md).

---

## ğŸ’¡ Models

| Model Variant          | Description                                    |
|------------------------|------------------------------------------------|
| `LLaMA-3.1-8B-Instruct` | Main model used for instruction tuning        |
| `LLaMA-3.2-1B-Instruct` | Smaller variant used for generalization study |

We use Unsloth + rsLoRA for efficient fine-tuning. Check `scripts/training` for configs.

---

## âš™ï¸ Environment

We recommend using Conda + pip.

```bash
conda create -n llama-events python=3.10
conda activate llama-events
pip install -r requirements.txt
```

Also run:

```bash
python -m spacy download en_core_web_lg
```

---

## ğŸš€ Running

### ğŸ‹ï¸ Training

```bash
python scripts/training/train.py --config configs/ace05.json
```

### ğŸ“Š Evaluation

```bash
python scripts/evaluation/eval.py --model_checkpoint path/to/checkpoint.pt
```


## ğŸ“ˆ Results

Check our full results and analysis in the [paper](https://arxiv.org/abs/2502.16377).  

---

## ğŸ§¾ Citation

If you find this work useful for your own research please cite our paper:

```bibtex
@misc{srivastava2025instruction,
  title={Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines},
  author={Saurabh Srivastava and Sweta Pati and Ziyu Yao},
  year={2025},
  eprint={2502.16377},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
```

---

## ğŸ“¬ Contact

For questions or contributions:
- Email: ssrivas6@gmu.edu | spati@gmu.edu | ziyuyao@gmu.edu
- GitHub Issues: Open one!

---

## ğŸ“œ License

This project is licensed under the MIT License.
